{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2aefb6a",
   "metadata": {},
   "source": [
    "# GPT-3\n",
    "* Notebook: [gpt3.ipynb](gpt3.ipynb)\n",
    "* Purpose: Access OpenAI API to receive model generated answers for prompted questions from SQuAD 1.1\n",
    "* Link to Paper: [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)\n",
    "* Adapted code: [Stanford's CS224U OpenQA Homework](https://github.com/cgpotts/cs224u/blob/master/hw_openqa.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51a495",
   "metadata": {},
   "source": [
    "**Paper Abstract**:<br/>\n",
    "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\n",
    "on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic\n",
    "in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of\n",
    "thousands of examples. By contrast, humans can generally perform a new language task from only\n",
    "a few examples or from simple instructions – something which current NLP systems still largely\n",
    "struggle to do. Here we show that scaling up language models greatly improves task-agnostic,\n",
    "few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion\n",
    "parameters, 10x more than any previous non-sparse language model, and test its performance in\n",
    "the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning,\n",
    "with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3\n",
    "achieves strong performance on many NLP datasets, including translation, question-answering, and\n",
    "cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as\n",
    "unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\n",
    "time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some\n",
    "datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\n",
    "we find that GPT-3 can generate samples of news articles which human evaluators have difficulty\n",
    "distinguishing from articles written by humans. We discuss broader societal impacts of this finding\n",
    "and of GPT-3 in general.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9222aaca",
   "metadata": {},
   "source": [
    "![unsupervised_learning.png](./images/gpt3/unsupervised_learning.png)\n",
    "**Source**: OpenAI paper, page 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1141366b",
   "metadata": {},
   "source": [
    "![in_context_learning.png](./images/gpt3/in_context_learning.png)\n",
    "**Source**: OpenAI paper, page 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0c7bb8",
   "metadata": {},
   "source": [
    "## Install Dependencies and Set up SQuAD Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799ac7b",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169bf690",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.10.0\n",
    "!pip install ujson\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install spacy\n",
    "!pip install gitpython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36185a48",
   "metadata": {},
   "source": [
    "### SQuAD Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96209ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import namedtuple\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = load_dataset(\"squad\")\n",
    "SquadExample = namedtuple(\"SquadExample\",  \"id title context question answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_squad_split(squad, split=\"validation\"):\n",
    "    \"\"\"\n",
    "    Use `split='train'` for the train split.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of SquadExample named tuples with attributes\n",
    "    id, title, context, question, answers\n",
    "    \n",
    "    \"\"\"    \n",
    "    fields = squad[split].features\n",
    "    data = zip(*[squad[split][field] for field in fields])\n",
    "    return [SquadExample(eid, title, context, question, answers[\"text\"]) \n",
    "            for eid, title, context, question, answers in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f333f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train = get_squad_split(squad, split=\"train\")\n",
    "squad_dev = get_squad_split(squad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f50a6",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "Reused from CS224U's HW_OpenQA (Github repo linked above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eafbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_generated_answer(tokens, newline=\"\\n\" ): \n",
    "    \"\"\"Our LMs tend to insert initial newline characters before\n",
    "    they begin generating text. This function ensures that we \n",
    "    properly capture the true first line as the answer while\n",
    "    also ensuring that token probabilities are aligned.\"\"\"        \n",
    "    answer_token_indices = []\n",
    "    char_seen = False            \n",
    "    for i, tok in enumerate(tokens):\n",
    "        # This is the main condition: a newline that isn't an initial\n",
    "        # string of newlines:\n",
    "        if tok == newline and char_seen:\n",
    "            break\n",
    "        # Keep the initial newlines for consistency:\n",
    "        elif tok == newline and not char_seen:\n",
    "            answer_token_indices.append(i)\n",
    "        # Proper tokens:\n",
    "        elif tok != newline:\n",
    "            char_seen = True\n",
    "            answer_token_indices.append(i)\n",
    "    return answer_token_indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af930aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def get_tokens(s: str) -> List[str]:\n",
    "    \"\"\"Normalize string and split string into tokens.\"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "\n",
    "def compute_exact(a_gold: str, a_pred: str) -> int:\n",
    "    \"\"\"Compute the Exact Match score.\"\"\"\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "\n",
    "def compute_f1_from_tokens(gold_toks: List[str], pred_toks: List[str]) -> float:\n",
    "    \"\"\"Compute the F1 score from tokenized gold answer and prediction.\"\"\"\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def compute_f1(a_gold: str, a_pred: str) -> float:\n",
    "    \"\"\"Compute the F1 score.\"\"\"\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    return compute_f1_from_tokens(gold_toks, pred_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14f590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(examples, prompts, gens):\n",
    "    \"\"\"Generic evalution function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    examples: iterable of `SquadExample` instances\n",
    "    prompts: list of str\n",
    "    preds: list of LM-generated texts to evaluate as answers\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys \"em_per\", \"macro_f1\", \"examples\", where\n",
    "    each \"examples\" value is a dict\n",
    "    \n",
    "    \"\"\"        \n",
    "    results = []\n",
    "    for ex, prompt, gen in zip(examples, prompts, gens):\n",
    "        answers = ex.answers\n",
    "        pred = gen['generated_answer']\n",
    "        # The result is the highest EM from the available answer strings:\n",
    "        em = max([compute_exact(ans, pred) for ans in answers])\n",
    "        f1 = max([compute_f1(ans, pred) for ans in answers])\n",
    "        gen.update({\n",
    "            \"id\": ex.id, \n",
    "            \"question\": ex.question, \n",
    "            \"prediction\": pred, \n",
    "            \"answers\": answers, \n",
    "            \"em\": em,\n",
    "            \"f1\": f1\n",
    "        })\n",
    "        results.append(gen)\n",
    "    data = {}        \n",
    "    data[\"macro_f1\"] = np.mean([d['f1'] for d in results])\n",
    "    data[\"em_per\"] = sum([d['em'] for d in results]) / len(results)\n",
    "    data[\"examples\"] = results\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5221e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = namedtuple(\"SquadExample\",  \"id title context question answers\")\n",
    "\n",
    "examples = [\n",
    "    ex(\"0\", \"CS224u\", \n",
    "       \"The course to take is NLU!\", \n",
    "       \"What is the course to take?\", \n",
    "       [\"NLU\", \"CS224u\"])]\n",
    "\n",
    "prompts = [\"Dear model, Please answer this question!\\n\\nQ: What is the course to take?\\n\\nA:\"]\n",
    "\n",
    "gens = [{\"generated_answer\": \"NLU\", \"generated_text\": \"NLU\\nWho am I?\"}]\n",
    "\n",
    "evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbefc1e",
   "metadata": {},
   "source": [
    "## Set Up OpenAI API Access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca13506",
   "metadata": {},
   "source": [
    "To use the GPT-3 function, sign up for an OpenAI account at https://beta.openai.com/signup<br/><br/>\n",
    "That should give you $18 in free credits.<br/><br/>\n",
    "Once your account is set up, you can get your API key from your account dashboard and paste it in below as the value of `openai_key`.<br/><br/>\n",
    "Instructions from HW_OpenQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = \"PUT YOUR OPENAI KEY HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0250ff",
   "metadata": {},
   "source": [
    "## Language Model Setup\n",
    "To use the GPT-3 API, install the OpenAI library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f24784",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df3913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gpt3(prompts, engine=\"text-curie-001\", temperature=0.1, top_p=0.95, **gpt3_kwargs):\n",
    "    \"\"\"To use this function, sign up for an OpenAI account at\n",
    "        \n",
    "    https://beta.openai.com/signup\n",
    "    \n",
    "    That should give you $18 in free credits, which is more than enough\n",
    "    for this assignment assuming you are careful with testing.\n",
    "    \n",
    "    Once your account is set up, you can get your API key from your \n",
    "    account dashboard and paste it in below as the value of \n",
    "    `openai.api_key`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prompts : iterable of str\n",
    "    engine : str\n",
    "        This has to be one of the models whose name begins with \"text\".\n",
    "        The \"instruct\" class of models can't be used, since they seem\n",
    "        to depend on some kinds of QA-relevant supervision.        \n",
    "        For options, costs, and other details: \n",
    "        https://beta.openai.com/docs/engines/gpt-3                \n",
    "    temperature : float\n",
    "        It seems best to set it low for this task!\n",
    "    top_p : float\n",
    "        \n",
    "    For information about values for `gpt3_kwargs`, see\n",
    "    \n",
    "    https://beta.openai.com/docs/api-reference/completions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts   \n",
    "    \n",
    "    \"\"\"\n",
    "    # Fill this in with the value from your OpenAI account. First\n",
    "    # verify that your account is set up with a spending limit that\n",
    "    # you are comfortable with. If you just opened your account,\n",
    "    # you should have $18 in credit and so won't need to supply any\n",
    "    # payment information.\n",
    "    openai.api_key = openai_key\n",
    "    \n",
    "    \n",
    "    assert engine.startswith(\"text\"), \\\n",
    "        \"Please use an engine whose name begins with 'text'.\"\n",
    "        \n",
    "    response = openai.Completion.create(\n",
    "        engine=engine,       \n",
    "        prompt=prompts,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        echo=False,   # This function will not work\n",
    "        logprobs=1,   # properly if any of these\n",
    "        n=1,          # are changed!\n",
    "        **gpt3_kwargs)\n",
    "    \n",
    "    # From here, we parse each example to get the values\n",
    "    # we need:\n",
    "    data = []\n",
    "    for ex, prompt in zip(response[\"choices\"], prompts):\n",
    "        tokens = ex[\"logprobs\"][\"tokens\"]\n",
    "        logprobs = ex[\"logprobs\"][\"token_logprobs\"]        \n",
    "        probs = list(np.exp(logprobs))\n",
    "        if \"<|endoftext|>\" in tokens:\n",
    "            end_i = tokens.index(\"<|endoftext|>\")\n",
    "            tokens = tokens[ : end_i]  # This leaves off the \"<|endoftext|>\"\n",
    "            probs = probs[ : end_i]    # token -- perhaps dubious.\n",
    "        ans_indices = _find_generated_answer(tokens)\n",
    "        answer_tokens = [tokens[i] for i in ans_indices]\n",
    "        answer_probs = [probs[i] for i in ans_indices]\n",
    "        answer = \"\".join(answer_tokens)        \n",
    "        data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_text\": ex[\"text\"],\n",
    "            \"generated_tokens\": tokens,\n",
    "            \"generated_probs\": probs,\n",
    "            \"generated_answer\": answer,\n",
    "            \"generated_answer_tokens\": answer_tokens,\n",
    "            \"generated_answer_probs\": answer_probs})\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_ex = run_gpt3([\n",
    "     \"What year was Stanford University founded?\",\n",
    "     \"In which year did Stanford first enroll students?\"])\n",
    "\n",
    "gpt3_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed09c43a",
   "metadata": {},
   "source": [
    "## Functions to Built and Evaluate Few Shot QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_few_shot_qa_prompt(ex, squad_train, n_context=2, joiner=\"\\n\\n\"):\n",
    "    segs = []\n",
    "    train_exs = random.sample(squad_train, k=n_context)    \n",
    "    for t in train_exs:\n",
    "        segs += [\n",
    "            f\"Title: {t.title}\",\n",
    "            f\"Background: {t.context}\",\n",
    "            f\"Q: {t.question}\",\n",
    "            f\"A: {t.answers[0]}\"\n",
    "        ]\n",
    "    segs += [\n",
    "        f\"Title: {ex.title}\",\n",
    "        f\"Background: {ex.context}\",\n",
    "        f\"Q: {ex.question}\",\n",
    "        f\"A:\"\n",
    "    ]\n",
    "    return joiner.join(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a89ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(build_few_shot_qa_prompt(squad_dev[0], squad_train, n_context=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b44764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_few_shot_qa(examples, squad_train, gen_func=run_gpt3, batch_size=20, n_context=2):\n",
    "    prompts = []\n",
    "    gens = []\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        batch = examples[i: i+batch_size]\n",
    "        ps = [build_few_shot_qa_prompt(ex, squad_train, n_context=n_context) for ex in batch]        \n",
    "        gs = gen_func(ps)       \n",
    "        prompts += ps\n",
    "        gens += gs\n",
    "        time.sleep(10) # GPT rate limit exceeded if more than 60 requests received in a minute\n",
    "    return evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0305147a",
   "metadata": {},
   "source": [
    "## Evaluate SQuAD Dev Set Using GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ce3af",
   "metadata": {},
   "source": [
    "### NOTE: Running These Cells Will Charge You around 9 Dollars in OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "few_shot_qa_results_gpt3 = evaluate_few_shot_qa(\n",
    "    squad_dev, squad_train, n_context=1, gen_func=run_gpt3)\n",
    "\n",
    "print(few_shot_qa_results_gpt3['macro_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc159632",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Macro F1: \" + str(few_shot_qa_results_gpt3['macro_f1']))\n",
    "print(\"Exact Match: \" + str(few_shot_qa_results_gpt3[\"em_per\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
