{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53106649",
   "metadata": {},
   "source": [
    "# QA-GNN\n",
    "* Notebook: [qagnn.ipynb](qagnn.ipynb)\n",
    "* Purpose: Adaptation of the QA-GNN model to perform multiple choice question answering from SQuAD 1.1\n",
    "* Link to Paper: [QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering](https://arxiv.org/abs/2104.06378)\n",
    "* Adapted code: [Stanford Official Implementation](https://github.com/michiyasunaga/qagnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e687e5",
   "metadata": {},
   "source": [
    "**Paper Abstract**<br/>\n",
    "The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. In this work, we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph neural networks. We evaluate our model on QA benchmarks in the commonsense (CommonsenseQA, OpenBookQA) and biomedical (MedQA-USMLE) domains. QA-GNN outperforms existing LM and LM+KG models, and exhibits capabilities to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03fca2c",
   "metadata": {},
   "source": [
    "![task.png](./images/qagnn/task.png)\n",
    "![overview.png](./images/qagnn/overview.png)\n",
    "**Source**: QA-GNN Github README"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3428c",
   "metadata": {},
   "source": [
    "### Install Dependencies and Import QA-GNN Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install transformers==3.4.0\n",
    "!pip install nltk spacy==2.1.6\n",
    "!python -m spacy download en\n",
    "\n",
    "# for torch-geometric\n",
    "!pip install torch-scatter==2.0.7 -f https://pytorch-geometric.com/whl/torch-1.8.1+cu111.html\n",
    "!pip install torch-sparse==0.6.9 -f https://pytorch-geometric.com/whl/torch-1.8.1+cu111.html\n",
    "!pip install torch-geometric==1.7.0 -f https://pytorch-geometric.com/whl/torch-1.8.1+cu111.html\n",
    "    \n",
    "# file utilities\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a71809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "\n",
    "qa_gnn_repo_path = \"qagnn/\"\n",
    "base_dir = \"/home/scpdxcs/\"\n",
    "\n",
    "if not os.path.exists(base_dir + qa_gnn_repo_path):\n",
    "    !git clone https://github.com/michiyasunaga/qagnn.git\n",
    "        \n",
    "# from https://stackoverflow.com/questions/4383571/importing-files-from-different-folder\n",
    "sys.path.append(qa_gnn_repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745dfb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from qagnn import *\n",
    "from modeling.modeling_qagnn import *\n",
    "from utils.optimization_utils import OPTIMIZER_CLASSES\n",
    "from utils.parser_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d17ead4",
   "metadata": {},
   "source": [
    "## Download QA-GNN data\n",
    "This section has been adapted from the QA-GNN README found here: https://github.com/michiyasunaga/qagnn/blob/main/README.md\n",
    "\n",
    "It has been adapted to do the following:\n",
    "* Used the pre-processed question answering datasets provided by QA-GNN researchers: *CommonsenseQA*, *OpenBookQA* and the ConceptNet knowledge graph.\n",
    "* Convert shell scripts to Python statements which can be run in Jupyter\n",
    "* If the preprocessed zip file exists, skip re-downloading\n",
    "* If data has already been expanded into a directory, archive that directory and re-expand (following the shell script's logic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# source: https://stackoverflow.com/questions/58125279/python-wget-module-doesnt-show-progress-bar\n",
    "def bar_progress(current, total, width=80):\n",
    "  progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
    "  sys.stdout.write(\"\\r\" + progress_message)\n",
    "  sys.stdout.flush()\n",
    "\n",
    "# Archive old data into backup folder\n",
    "old_data_path = qa_gnn_repo_path + \"data_old/\"\n",
    "new_data_path = qa_gnn_repo_path + \"data/\"\n",
    "\n",
    "if os.path.exists(new_data_path):\n",
    "    if os.path.exists(old_data_path):\n",
    "        shutil.rmtree(old_data_path)\n",
    "    shutil.move(new_data_path, old_data_path)\n",
    "\n",
    "# Download and stage CommonsenseQA, OpenBookQA and the ConceptNet knowledge graph\n",
    "url = \"https://nlp.stanford.edu/projects/myasu/QAGNN/data_preprocessed_release.zip\"\n",
    "save_path = qa_gnn_repo_path + \"data_preprocessed_release.zip\"\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    wget.download(url, save_path, bar=bar_progress)\n",
    "\n",
    "with zipfile.ZipFile(save_path,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(new_data_path)\n",
    "    \n",
    "## Download and stage MedQA-USMLE\n",
    "#biomed_url = \"https://nlp.stanford.edu/projects/myasu/QAGNN/data_preprocessed_biomed.zip\"\n",
    "#biomed_save_path = qa_gnn_repo_path + \"data_preprocessed_biomed.zip\"\n",
    "\n",
    "#if not os.path.exists(biomed_save_path):\n",
    "#    wget.download(biomed_url, biomed_save_path, bar=bar_progress)\n",
    "\n",
    "#with zipfile.ZipFile(biomed_save_path,\"r\") as zip_ref:\n",
    "#    zip_ref.extractall(new_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b97205",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ~/qagnn/data_preprocessed_release.zip\n",
    "#!rm ~/qagnn/data_preprocessed_biomed.zip\n",
    "!mv ~/qagnn/data/data_preprocessed_release/* ~/qagnn/data\n",
    "#!mv ~/qagnn/data/data_preprocessed_biomed/* ~/qagnn/data\n",
    "!rm -rf ~/qagnn/data/data_preprocessed_release/*\n",
    "#!rm -rf ~/qagnn/data/data_preprocessed_biomed/*\n",
    "!cp -R ~/saved_datasets/squad ~/qagnn/data/squad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53ef5c",
   "metadata": {},
   "source": [
    "### Create Common Train and Eval Functions for Argument Generation\n",
    "In the Git repo and associated documentation, training and evaluation has been set up through a series of shell scripts. To test training and evaluation within Jupyter, I've adapted run_qagnn__obqa.sh into the following Python cell block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def add_train_defaults(args):\n",
    "    args.k = 5\n",
    "    args.gnn_dim = 200\n",
    "    args.elr = \"1e-5\"\n",
    "    args.decoder_lr = \"1e-3\"\n",
    "    args.bs = 128\n",
    "    args.mbs = 1\n",
    "    args.fp16 = True\n",
    "    args.seed = 0\n",
    "    # num_relation: (17 +2) * 2: originally 17, add 2 relation types \n",
    "    # (QA context -> Q node; QA context -> A node), and double because we add reverse edges\n",
    "    args.num_relation = 38\n",
    "    args.n_epochs = 100\n",
    "    args.max_epochs_before_stop = 50\n",
    "    args.save_model = True\n",
    "    args = add_save_dir(args)\n",
    "    return args\n",
    "\n",
    "def add_eval_defaults(args, model_name):\n",
    "    # setting save_model to False leads to evaluation being printed to terminal instead of saved to csv\n",
    "    args.save_model = True\n",
    "    saved_models_path = \"saved_models/\"\n",
    "    args.load_model_path = saved_models_path + model_name + \"_model_hf3.4.0.pt\"\n",
    "    args.save_dir = \"saved_models\"\n",
    "    \n",
    "    return args\n",
    "\n",
    "# in train shell scripts, locations to find training data\n",
    "def add_dataset_defaults(args):\n",
    "    dataset_data_dir = \"data/\" + args.dataset + \"/\"\n",
    "    args.train_adj = dataset_data_dir + \"graph/train.graph.adj.pk\"\n",
    "    args.dev_adj = dataset_data_dir + \"graph/dev.graph.adj.pk\"\n",
    "    args.test_adj = dataset_data_dir + \"graph/test.graph.adj.pk\"\n",
    "    args.train_statements = dataset_data_dir + \"statement/train.statement.jsonl\"\n",
    "    args.dev_statements = dataset_data_dir + \"statement/dev.statement.jsonl\"\n",
    "    args.test_statements = dataset_data_dir + \"statement/test.statement.jsonl\"\n",
    "    return args\n",
    "\n",
    "# qagnn.py argument defaults in main function\n",
    "def add_main_defaults(args):\n",
    "    args.load_model_path = None\n",
    "    args.use_cache = True\n",
    "    args.att_head_num = 2\n",
    "    args.fc_dim = 200\n",
    "    args.fc_layer_num = 0\n",
    "    args.freeze_ent_emb = True\n",
    "    args.max_node_num = 200\n",
    "    args.simple = False\n",
    "    args.subsample = 1.0\n",
    "    args.init_range = 0.02\n",
    "    args.dropouti = 0.2\n",
    "    args.dropoutg = 0.2\n",
    "    args.dropoutf = 0.2\n",
    "    args.eval_batch_size = 1\n",
    "    args.unfreeze_epoch = 4\n",
    "    args.refreeze_epoch = 10000\n",
    "    args.drop_partial_batch = False\n",
    "    args.fill_partial_batch = False\n",
    "    return args\n",
    "\n",
    "def add_save_dir(args):\n",
    "    save_dir_pref = \"saved_models\"\n",
    "    now = datetime.now()\n",
    "    dt = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # create save_dir_pref directory if it does not exist\n",
    "    if not os.path.exists(save_dir_pref):\n",
    "        os.mkdir(save_dir_pref)\n",
    "    \n",
    "    train_experiment_str = \"enc-\" + args.encoder + \"__k\" + str(args.k) + \"__gnndim\" + str(args.gnn_dim) + \"__bs\" + str(args.bs) + \"__seed\" + str(args.seed) + \"__\" + dt\n",
    "    args.save_dir = os.path.join(save_dir_pref, dataset, train_experiment_str)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f079cd0",
   "metadata": {},
   "source": [
    "### Load Pre-Trained Model for Evaluation\n",
    "As of October 20, 2022, the QAGNN team has three pre-built models available for download:<br/>\n",
    "* RoBERTa-large + QA-GNN (trained on Common Sense QA): https://nlp.stanford.edu/projects/myasu/QAGNN/models/csqa_model_hf3.4.0.pt\n",
    "* RoBERTa-large + QA-GNN (trained on Open Book QA): https://nlp.stanford.edu/projects/myasu/QAGNN/models/obqa_model_hf3.4.0.pt\n",
    "* SapBERT-base + QA-GNN (trained on MedQA-USMLE): https://nlp.stanford.edu/projects/myasu/QAGNN/models/medqa_usmle_model_hf3.4.0.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e85ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "# three choices for pretrained model: csqa, obqa, medqa_usmle\n",
    "pretrained_model = \"csqa\"\n",
    "pretrained_model_file = pretrained_model + \"_model_hf3.4.0.pt\"\n",
    "\n",
    "# prep paths for URL and download location\n",
    "saved_models_path = base_dir + qa_gnn_repo_path + \"saved_models/\"\n",
    "pretrained_model_path = saved_models_path + pretrained_model_file\n",
    "pretrained_url = \"https://nlp.stanford.edu/projects/myasu/QAGNN/models/\" + pretrained_model_file\n",
    "!mkdir \"/home/scpdxcs/qagnn/saved_models/\"\n",
    "\n",
    "if not os.path.exists(pretrained_model_path):\n",
    "    wget.download(pretrained_url, pretrained_model_path, bar=bar_progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00980de",
   "metadata": {},
   "source": [
    "## Optional: Train a Model from a Dataset and Evaluate It"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c80d5e",
   "metadata": {},
   "source": [
    "### Train a Model from a Dataset (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f11d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_parser()\n",
    "args, _ = parser.parse_known_args()\n",
    "args.dataset = \"obqa\"\n",
    "args.encoder = \"roberta-large\"\n",
    "args = add_main_defaults(args)\n",
    "args = add_train_defaults(args)\n",
    "args = add_dataset_defaults(args)\n",
    "\n",
    "os.chdir(\"qagnn\")\n",
    "qagnn.train(args)\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a7dff",
   "metadata": {},
   "source": [
    "### Evaluate a Trained Model (Optional)\n",
    "This block is here to show how to use a pretrained model with a pre-processed CSQA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb43aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_parser()\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "args.dataset = pretrained_model\n",
    "args = add_main_defaults(args)\n",
    "args = add_eval_defaults(args)\n",
    "args = add_dataset_defaults(args)\n",
    "\n",
    "os.chdir(\"qagnn\")\n",
    "qagnn.eval_detail(args)\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c6fc89",
   "metadata": {},
   "source": [
    "# Evaluate a Trained Model with QA-GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40fe4e",
   "metadata": {},
   "source": [
    "**parser_utils.py updates**:<br/>\n",
    "<br/>\n",
    "**Add to ENCODER_DEFAULT_LR**:<br/>\n",
    "     'squad': {<br/>\n",
    "         'lstm': 3e-4,<br/>\n",
    "         'openai-gpt': 1e-4,<br/>\n",
    "         'bert-base-uncased': 3e-5,<br/>\n",
    "         'bert-large-uncased': 2e-5,<br/>\n",
    "         'roberta-large': 1e-5,<br/>\n",
    "     },<br/>\n",
    "<br/>\n",
    "**Update DATASET_LIST** = ['csqa', 'obqa', 'squad', 'socialiqa', 'medqa_usmle']<br/>\n",
    "<br/>\n",
    "**Add to DATASET_SETTING**<br/>\n",
    " DATASET_SETTING = {<br/>\n",
    "     'csqa': 'inhouse',<br/>\n",
    "     'obqa': 'official',<br/>\n",
    "     'squad': 'official',<br/>\n",
    "     'socialiqa': 'official',<br/>\n",
    "     'medqa_usmle': 'official',<br/>\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy parser_utils updates into place\n",
    "!cp /home/scpdxcs/utils/parser_utils_squad.py /home/scpdxcs/qagnn/utils/parser_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167ee441",
   "metadata": {},
   "source": [
    "## Run Generate Multiple Choice Questions Notebook to Convert SQuAD Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf327af",
   "metadata": {},
   "source": [
    "* Notebook: [Generate_Multiple_Choice_Questions.ipynb](Generate_Multiple_Choice_Questions.ipynb)\n",
    "* Purpose: QA-GNN requires multiple choice answers to perform question answering. This notebook generates noun phrases that are semantically related (if they can be grounded by WordNet or ConceptNet) and creates the jsonl files required for QA-GNN to run.\n",
    "* Source Explanation: [Ramsri Goutham's Practical AI](https://towardsdatascience.com/practical-ai-automatically-generate-multiple-choice-questions-mcqs-from-any-content-with-bert-2140d53a9bf5)\n",
    "* Adapted code: [Generate_MCQ_BERT_Wordnet_Conceptnet Repo](https://github.com/ramsrigouthamg/Generate_MCQ_BERT_Wordnet_Conceptnet.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0873ab9",
   "metadata": {},
   "source": [
    "### Use the SQuAD test dataset as Train and Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /home/scpdxcs/qagnn/data/squad/graph/train.graph.adj.pk /home/scpdxcs/qagnn/data/squad/graph/dev.graph.adj.pk\n",
    "!cp /home/scpdxcs/qagnn/data/squad/graph/train.graph.adj.pk /home/scpdxcs/qagnn/data/squad/graph/test.graph.adj.pk\n",
    "!cp /home/scpdxcs/qagnn/data/squad/grounded/train.grounded.jsonl /home/scpdxcs/qagnn/data/squad/grounded/dev.grounded.jsonl\n",
    "!cp /home/scpdxcs/qagnn/data/squad/grounded/train.grounded.jsonl /home/scpdxcs/qagnn/data/squad/grounded/test.grounded.jsonl\n",
    "!cp /home/scpdxcs/qagnn/data/squad/statement/train.statement.jsonl /home/scpdxcs/qagnn/data/squad/statement/dev.statement.jsonl\n",
    "!cp /home/scpdxcs/qagnn/data/squad/statement/train.statement.jsonl /home/scpdxcs/qagnn/data/squad/statement/test.statement.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0946f",
   "metadata": {},
   "source": [
    "## Evaluate QA-GNN with SQuAD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c268cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_parser()\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "args.dataset = \"squad\"\n",
    "args = add_main_defaults(args)\n",
    "args = add_eval_defaults(args, \"csqa\")\n",
    "args = add_dataset_defaults(args)\n",
    "\n",
    "os.chdir(\"qagnn/\")\n",
    "eval_detail(args)\n",
    "os.chdir(\"..\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
